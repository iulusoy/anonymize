{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Batching Performance Testing\n",
    "\n",
    "This notebook is intended for testing the pseudonymization performance for different transformers batch sizes.\n",
    "\n",
    "Make sure to disable Windows Hibernate/Sleep when running long tests or the result will be falsified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mailcom.inout\n",
    "import mailcom.parse\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for displaying the result using HTML\n",
    "def highlight_ne(text, per_list, org_list, loc_list, misc_list):\n",
    "    # create a list of all entities with their positions\n",
    "    entities = []\n",
    "    for loc in loc_list:\n",
    "        entities.append((loc, \"green\"))\n",
    "    for org in org_list:\n",
    "        entities.append((org, \"blue\"))\n",
    "    for misc in misc_list:\n",
    "        entities.append((misc, \"yellow\"))\n",
    "    for per in per_list:\n",
    "        entities.append((per, \"red\"))\n",
    "    \n",
    "    # sort entities by their positions in the text in reverse order\n",
    "    entities.sort(key=lambda x: text.find(x[0]), reverse=True)\n",
    "    \n",
    "    # replace entities with highlighted spans\n",
    "    for entity, color in entities:\n",
    "        text = text.replace(entity, f\"<span style=\\\"background-color:{color}\\\">{entity}</span>\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function for a single performance test. The model is loaded from scratch every iteration, and the csv file is reread every time, to create equal conditions for every batching size. The text with pseudonymized entities can be displayed by setting ``disp=True``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_test(batch_size, disp=True):\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Starting performance test for batch size\", batch_size)\n",
    "    # create t0 timestamp\n",
    "    t0 = time.time()\n",
    "\n",
    "    # import files from csv file\n",
    "    email_list = pd.read_csv(\"../mailcom/test/data/mails_lb_sg_copy2.csv\")\n",
    "    #print(email_list)\n",
    "\n",
    "    t_csv_read = time.time()\n",
    "\n",
    "    # create pseudonymization object\n",
    "    ps = mailcom.parse.Pseudonymize()\n",
    "    ps.init_spacy(\"fr\")\n",
    "    ps.init_transformers()\n",
    "    ps.set_sentence_batch_size(batch_size)\n",
    "    # time stamp after model loading\n",
    "    t_model_loaded = time.time()\n",
    "\n",
    "    # loop over mails and pseudonymize them\n",
    "    out_list = []\n",
    "    ts_list = []\n",
    "    for idx, row in email_list.iterrows():\n",
    "        ts_email_start = time.time()\n",
    "        text = row[\"message\"]\n",
    "        email_dict = {\"content\": text}\n",
    "        if not text:\n",
    "            continue\n",
    "        # Test functionality of Pseudonymize class\n",
    "        # Pseudonymization is usually done using ps.pseudonymize\n",
    "        # For performance analysis the process is split into its subprocesses here\n",
    "        ps.reset()\n",
    "        sentences = ps.get_sentences(text)\n",
    "        batches = ps.split_batches(sentences)\n",
    "        ts_email_ppr_done = time.time()\n",
    "        pseudonymized_batches = []\n",
    "        for batch in batches:\n",
    "            batch = ps.concatenate(batch)\n",
    "            batch = ps.pseudonymize_email_addresses(batch)\n",
    "            ner = ps.get_ner(batch)\n",
    "            ps_sent = \" \".join(ps.pseudonymize_ne(ner, batch)) if ner else batch\n",
    "            ps_sent = ps.pseudonymize_numbers(ps_sent)\n",
    "            pseudonymized_batches.append(ps_sent)\n",
    "        output_text = ps.concatenate(pseudonymized_batches)\n",
    "\n",
    "        # add output to dict\n",
    "        email_dict[\"pseudo_content\"] = output_text\n",
    "        out_list.append(email_dict)\n",
    "\n",
    "        # timestamp after this email\n",
    "        ts_email_end = time.time()\n",
    "        ts_list.append([ts_email_start, ts_email_ppr_done, ts_email_end])\n",
    "\n",
    "        # display the pseudonymized text\n",
    "        # display(HTML(output_text))\n",
    "\n",
    "        # display original text and highlight found and replaced NEs\n",
    "        if disp:\n",
    "            highlighted_html = highlight_ne(text, ps.per_list, ps.org_list, ps.loc_list, ps.misc_list)\n",
    "            display(HTML(highlighted_html))\n",
    "\n",
    "    # display timestamps\n",
    "\n",
    "    # bar plot for each individual email\n",
    "    # processing times\n",
    "    idx_list = [row[0] for row in email_list.iterrows()]\n",
    "    email_duration_list = [ts[2] - ts[1] for ts in ts_list]\n",
    "    email_ppr_list = [ts[1] - ts[0] for ts in ts_list]\n",
    "    email_total_list = [ts[2] - ts[0] for ts in ts_list]\n",
    "    email_bar_height = {\n",
    "        \"Pre-Processing\": email_ppr_list,\n",
    "        \"Pseudonymization\": email_duration_list\n",
    "    }\n",
    "    bt = [0 for idx in idx_list]\n",
    "\n",
    "    plt.figure(figsize=(10,4), dpi=80)\n",
    "\n",
    "    # plot 1\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for key, height in email_bar_height.items():\n",
    "        plt.bar(idx_list, height, 0.5, label=key, bottom=bt)\n",
    "        bt = [bi + hi for (bi,hi) in zip(bt, height)]\n",
    "    plt.xlabel(\"Email\")\n",
    "    plt.ylabel(\"t [s]\")\n",
    "    plt.title(\"Computation times for emails, model loading and file reading\")\n",
    "    plt.legend()\n",
    "\n",
    "    # plot for model loading and file reading, as well as average email time\n",
    "    # processing times\n",
    "    bar_x = [\"CSV Reading\", \"Model Loading\", \"Average Email Time\"]\n",
    "    average_email_time = sum(email_total_list) / len(email_total_list)\n",
    "    bar_y = [t_csv_read - t0, t_model_loaded - t0, average_email_time]\n",
    "    plt.ylabel(\"t [s]\")\n",
    "\n",
    "    # plot 2\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(bar_x, bar_y, 0.5)\n",
    "\n",
    "    # Total time\n",
    "    print(\"Total time:\", (datetime.datetime.fromtimestamp(ts_list[len(ts_list)-1][2] - t_model_loaded).strftime('%M:%S')))\n",
    "\n",
    "    # plt.savefig(\"out/mailcom_batching_performance_n_\" + str(batch_size) + datetime.datetime.fromtimestamp(t0).strftime('%H%M%S') + \".png\")\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "    return average_email_time, out_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the testing setup is configured. The tested batching sizes are set in `batching_sizes`. For each batching size, `n_samples` independent runs are executed and the performances are averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batching_sizes = [-1, 1, 2, 3, 4, 6, 8, 10]\n",
    "# batching_sizes = [1]\n",
    "n_samples = 5\n",
    "outputs = []\n",
    "\n",
    "# first make a dummy run since there seem to be some inconsitencies when loading for the first time\n",
    "_ = performance_test(-1, disp=False)\n",
    "\n",
    "# testing\n",
    "av_email_times_for_batches = []\n",
    "for bs in batching_sizes:\n",
    "    average_email_time = 0\n",
    "    for sid in range(n_samples):\n",
    "        t, out = performance_test(bs, disp=False)\n",
    "        average_email_time += t\n",
    "        test_result_dict = {\n",
    "            \"batch_size\": bs,\n",
    "            \"sample\": sid,\n",
    "            \"email_outputs\": out,\n",
    "            \"average_email_time\": average_email_time\n",
    "        }\n",
    "        outputs.append(test_result_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bar plot displaying the average email processing times for the different batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_email_times_for_batches = [0]*len(batching_sizes)\n",
    "std_email_times_for_batches = [0]*len(batching_sizes)\n",
    "for output in outputs:\n",
    "    average_email_times_for_batches[batching_sizes.index(output[\"batch_size\"])] += output[\"average_email_time\"]\n",
    "average_email_times_for_batches = [avt/n_samples for avt in average_email_times_for_batches]\n",
    "\n",
    "# if n_samples > 1, calculate standard deviation.\n",
    "if n_samples > 1:\n",
    "    for output in outputs:\n",
    "        ix = batching_sizes.index(output[\"batch_size\"])\n",
    "        std_email_times_for_batches[ix] += (output[\"average_email_time\"] - average_email_times_for_batches[ix])**2\n",
    "    std_email_times_for_batches = [(stdt**(1./2.))/(n_samples-1) for stdt in std_email_times_for_batches]\n",
    "\n",
    "plt.errorbar(batching_sizes, average_email_times_for_batches, yerr=std_email_times_for_batches, linestyle='None', marker='.', capsize=2, elinewidth=1)\n",
    "plt.xlabel(\"n batches\")\n",
    "plt.ylabel(\"Average Email Time [s]\")\n",
    "plt.ylim(bottom=0)\n",
    "plt.title(\"Average email time for different batch sizes\")\n",
    "plt.grid(which='major', color='#666666', linestyle='--', alpha = 0.8)\n",
    "plt.grid(which='minor', color='#666666', linestyle='--', alpha = 0.3)\n",
    "plt.minorticks_on()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for quality differences by displaying differences in the pseudonymized text using ``difflib``. Deltas are only printed if there are differences between the pseudonymized texts. A cross-check is performed using ``difflib.SequenceMatcher().ratio()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare a result as standard to compare the other results to\n",
    "standard_ps_texts = [email_dict[\"pseudo_content\"] for email_dict in next(output[\"email_outputs\"] for output in outputs if output[\"batch_size\"] == 1)]\n",
    "\n",
    "# iterate over other results and print diffs\n",
    "for output in outputs:\n",
    "    #print(f\"----- Comparing batch size {output['batch_size']} sample {output['sample']} to standard: -----\")\n",
    "    ps_texts = [email_dict[\"pseudo_content\"] for email_dict in output[\"email_outputs\"]]\n",
    "    # diff to standard\n",
    "    for idx, (text, stdtext) in enumerate(zip(ps_texts, standard_ps_texts)):\n",
    "        #print(f\"--- Comparing email text {idx} ---\")\n",
    "        diff = difflib.ndiff(stdtext.splitlines(keepends=True), text.splitlines(keepends=True))\n",
    "        for line in diff:\n",
    "            if line.startswith('+ ') or line.startswith('- '):\n",
    "                print(f\"Delta in batch size {output['batch_size']} at sample {output['sample']}:\")\n",
    "                print(line, end='')\n",
    "        # also test the matching ratio\n",
    "        rt = difflib.SequenceMatcher(None, stdtext, text).ratio()\n",
    "        if not rt == 1.0:\n",
    "            print(f\"Delta in batch size {output['batch_size']} at sample {output['sample']}: Matching ratio is not 1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mailcom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
