{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Batching Performance Testing\n",
    "\n",
    "This notebook is intended for testing the pseudonymization performance for different transformers batch sizes.\n",
    "\n",
    "Make sure to disable Windows Hibernate/Sleep when running long tests or the result will be falsified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mailcom.inout\n",
    "import mailcom.parse\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function for a single performance test. The model is loaded from scratch every iteration, and the csv file is reread every time, to create equal conditions for every batching size.\n",
    "\n",
    "The test returns the total email processing time averaged over all emails, as well as a list of all email output dictionaries as created in the pseudonymization:\n",
    "\n",
    "Email dict structure: |\n",
    "`content`: Original email text |\n",
    "`pseudo_content`: Pseudonymized email text |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_test(csv_file, batch_size):\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Starting performance test for batch size\", batch_size)\n",
    "    # create t0 timestamp\n",
    "    t0 = time.time()\n",
    "\n",
    "    # import files from csv file\n",
    "    email_list = pd.read_csv(csv_file)\n",
    "    t_csv_read = time.time()\n",
    "\n",
    "    # create pseudonymization object\n",
    "    ps = mailcom.parse.Pseudonymize()\n",
    "    ps.init_spacy(\"fr\")\n",
    "    ps.init_transformers()\n",
    "    ps.set_sentence_batch_size(batch_size)\n",
    "    # time stamp after model loading\n",
    "    t_model_loaded = time.time()\n",
    "\n",
    "    # loop over mails and pseudonymize them\n",
    "    out_list = []\n",
    "    ts_list = []\n",
    "    for idx, row in email_list.iterrows():\n",
    "        # email start time\n",
    "        ts_email_start = time.time()\n",
    "        text = row[\"message\"]\n",
    "        email_dict = {\"content\": text}\n",
    "        if not text:\n",
    "            continue\n",
    "        # Pseudonymization is usually done using ps.pseudonymize\n",
    "        # For performance analysis the process is split into its subprocesses here\n",
    "        ps.reset()\n",
    "        sentences = ps.get_sentences(text)\n",
    "        batches = ps.split_batches(sentences)\n",
    "        ts_email_ppr_done = time.time() # preprocessing complete\n",
    "        pseudonymized_batches = []\n",
    "        for batch in batches:\n",
    "            batch = ps.concatenate(batch)\n",
    "            batch = ps.pseudonymize_email_addresses(batch)\n",
    "            ner = ps.get_ner(batch)\n",
    "            ps_sent = \" \".join(ps.pseudonymize_ne(ner, batch)) if ner else batch\n",
    "            ps_sent = ps.pseudonymize_numbers(ps_sent)\n",
    "            pseudonymized_batches.append(ps_sent)\n",
    "        output_text = ps.concatenate(pseudonymized_batches)\n",
    "\n",
    "        # add output to dict\n",
    "        email_dict[\"pseudo_content\"] = output_text\n",
    "        out_list.append(email_dict)\n",
    "\n",
    "        # timestamp after this email\n",
    "        ts_email_end = time.time()\n",
    "        ts_list.append([ts_email_start, ts_email_ppr_done, ts_email_end])\n",
    "\n",
    "    # display timestamps\n",
    "\n",
    "    # bar plot for each individual email\n",
    "    # processing times\n",
    "    idx_list = [row[0] for row in email_list.iterrows()]\n",
    "    email_duration_list = [ts[2] - ts[1] for ts in ts_list]\n",
    "    email_ppr_list = [ts[1] - ts[0] for ts in ts_list]\n",
    "    email_total_list = [ts[2] - ts[0] for ts in ts_list]\n",
    "    email_bar_height = {\n",
    "        \"Pre-Processing\": email_ppr_list,\n",
    "        \"Pseudonymization\": email_duration_list\n",
    "    }\n",
    "    bt = [0 for idx in idx_list]\n",
    "\n",
    "    plt.figure(figsize=(10,4), dpi=80)\n",
    "\n",
    "    # plot 1\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for key, height in email_bar_height.items():\n",
    "        plt.bar(idx_list, height, 0.5, label=key, bottom=bt)\n",
    "        bt = [bi + hi for (bi,hi) in zip(bt, height)]\n",
    "    plt.xlabel(\"Email\")\n",
    "    plt.ylabel(\"t [s]\")\n",
    "    plt.title(\"Computation times for emails, model loading and file reading\")\n",
    "    plt.legend()\n",
    "\n",
    "    # plot for model loading and file reading, as well as average email time\n",
    "    # processing times\n",
    "    bar_x = [\"CSV Reading\", \"Model Loading\", \"Average Email Time\"]\n",
    "    average_email_time = sum(email_total_list) / len(email_total_list)\n",
    "    bar_y = [t_csv_read - t0, t_model_loaded - t0, average_email_time]\n",
    "    plt.ylabel(\"t [s]\")\n",
    "\n",
    "    # plot 2\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(bar_x, bar_y, 0.5)\n",
    "\n",
    "    # Total time\n",
    "    print(\"Total time:\", (datetime.datetime.fromtimestamp(ts_list[len(ts_list)-1][2] - t_model_loaded).strftime('%M:%S')))\n",
    "\n",
    "    # plt.savefig(\"out/mailcom_batching_performance_n_\" + str(batch_size) + datetime.datetime.fromtimestamp(t0).strftime('%H%M%S') + \".png\")\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "    return average_email_time, out_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the testing setup is configured. The tested batching sizes are set in `batching_sizes`. For each batching size, `n_samples` independent runs are executed and the performances are averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batching_sizes = [-1, 1, 2, 3, 4, 6, 8, 10]\n",
    "# batching_sizes = [1, 10]\n",
    "n_samples = 5\n",
    "outputs = []\n",
    "csv_file = \"../mailcom/test/data/mails_lb_sg.csv\"\n",
    "\n",
    "# first make a dummy run since there seem to be some inconsitencies when loading for the first time\n",
    "_ = performance_test(csv_file, -1)\n",
    "\n",
    "# testing\n",
    "for bs in batching_sizes:\n",
    "    for sid in range(n_samples):\n",
    "        t, out = performance_test(csv_file, bs)\n",
    "        test_result_dict = {\n",
    "            \"batch_size\": bs,\n",
    "            \"sample\": sid,\n",
    "            \"email_outputs\": out,\n",
    "            \"average_email_time\": t\n",
    "        }\n",
    "        outputs.append(test_result_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bar plot displaying the average email processing times for the different batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for email-averaged total processing time, now to be averaged over n_samples\n",
    "average_email_times_for_batches = [0]*len(batching_sizes)\n",
    "# corresponding sample standard deviations\n",
    "std_email_times_for_batches = [0]*len(batching_sizes)\n",
    "# manual computation of the average over n_samples\n",
    "for output in outputs:\n",
    "    # add times to list position corresponding to batch size\n",
    "    average_email_times_for_batches[batching_sizes.index(output[\"batch_size\"])] += output[\"average_email_time\"]\n",
    "# divide by length\n",
    "average_email_times_for_batches = [avt/n_samples for avt in average_email_times_for_batches]\n",
    "\n",
    "# if n_samples > 1, calculate sample standard deviation\n",
    "if n_samples > 1:\n",
    "    for output in outputs:\n",
    "        # add squared time deviations to list position corresponding to batch size\n",
    "        ix = batching_sizes.index(output[\"batch_size\"])\n",
    "        std_email_times_for_batches[ix] += (output[\"average_email_time\"] - average_email_times_for_batches[ix])**2\n",
    "    # divide by length-1\n",
    "    std_email_times_for_batches = [(stdt**(1./2.))/(n_samples-1) for stdt in std_email_times_for_batches]\n",
    "\n",
    "# plot\n",
    "plt.errorbar(batching_sizes, average_email_times_for_batches, yerr=std_email_times_for_batches, linestyle='None', marker='.', capsize=2, elinewidth=1)\n",
    "plt.xlabel(\"batch size n\")\n",
    "plt.ylabel(\"Average Email Time [s]\")\n",
    "plt.ylim(bottom=0)\n",
    "plt.title(\"Average email time for different batch sizes\")\n",
    "plt.vlines(0, 0, (max(average_email_times_for_batches) + max(std_email_times_for_batches) + 10), colors=\"gray\", linestyles=\"--\")\n",
    "plt.grid(which='major', color='#666666', linestyle='--', alpha = 0.8)\n",
    "plt.grid(which='minor', color='#666666', linestyle='--', alpha = 0.3)\n",
    "plt.minorticks_on()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for quality differences by displaying differences in the pseudonymized text using ``difflib``. Deltas are only printed if there are differences between the pseudonymized texts. Matching ratios are calculated using ``difflib.SequenceMatcher().ratio()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare a result as standard to compare the other results to\n",
    "standard_batch_size = 1 # batch size with best qualitative results\n",
    "standard_ps_texts = [email_dict[\"pseudo_content\"] for email_dict in next(output[\"email_outputs\"] for output in outputs if output[\"batch_size\"] == standard_batch_size)]\n",
    "\n",
    "# iterate over other results and print diffs\n",
    "for output in outputs:\n",
    "    print(f\"----- Comparing batch size {output['batch_size']} sample {output['sample']} to standard {standard_batch_size}: -----\")\n",
    "    # pseudonymized texts for this output\n",
    "    ps_texts = [email_dict[\"pseudo_content\"] for email_dict in output[\"email_outputs\"]]\n",
    "    # diff to standard\n",
    "    average_sqm_ratio = 0. # SequenceMatcherRatio averaged over all emails for this output\n",
    "    for idx, (text, stdtext) in enumerate(zip(ps_texts, standard_ps_texts)):\n",
    "        print(f\"--- Comparing email text {idx} ---\")\n",
    "        diff = difflib.ndiff(stdtext.splitlines(keepends=True), text.splitlines(keepends=True))\n",
    "        for line in diff:\n",
    "            if line.startswith('+ ') or line.startswith('- '):\n",
    "                print(f\"Delta in batch size {output['batch_size']} at sample {output['sample']}:\")\n",
    "                print(line, end='')\n",
    "        # also test the matching ratio\n",
    "        rt = difflib.SequenceMatcher(None, stdtext, text).ratio()\n",
    "        average_sqm_ratio += rt\n",
    "        if not rt == 1.0:\n",
    "            print(f\"Delta in batch size {output['batch_size']} at sample {output['sample']}: Matching ratio is {rt}\")\n",
    "\n",
    "    average_sqm_ratio = average_sqm_ratio / len(ps_texts)\n",
    "    output[\"average_sqm_ratio\"] = average_sqm_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the SequenceMatcher ratios over n_samples for all batching sizes\n",
    "average_sqm_ratio_for_batches = [0.]*len(batching_sizes)\n",
    "# manually average the ratios over n_samples\n",
    "for output in outputs:\n",
    "    average_sqm_ratio_for_batches[batching_sizes.index(output[\"batch_size\"])] += output[\"average_sqm_ratio\"]\n",
    "average_sqm_ratio_for_batches = [asr/n_samples for asr in average_sqm_ratio_for_batches]\n",
    "\n",
    "# if n_samples > 1, calculate standard deviation.\n",
    "std_sqm_ratio_for_batches = [0.]*len(batching_sizes)\n",
    "if n_samples > 1:\n",
    "    for output in outputs:\n",
    "        ix = batching_sizes.index(output[\"batch_size\"])\n",
    "        std_sqm_ratio_for_batches[ix] += (output[\"average_sqm_ratio\"] - average_sqm_ratio_for_batches[ix])**2\n",
    "    std_sqm_ratio_for_batches = [(stdr**(1./2.))/(n_samples-1) for stdr in std_sqm_ratio_for_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the average SequenceMatcher ratio for batch sizes\n",
    "plt.errorbar(batching_sizes, average_sqm_ratio_for_batches, yerr=std_sqm_ratio_for_batches, linestyle='None', marker='.', capsize=2, elinewidth=1)\n",
    "plt.xlabel(\"batch size n\")\n",
    "plt.ylabel(\"Average SequenceMatcher Ratio\")\n",
    "plt.title(\"Average SequenceMatcher Ratio compared to Standard for different batch sizes\")\n",
    "plt.hlines(1, -2, batching_sizes[len(batching_sizes)-1]+1, colors=\"black\")\n",
    "plt.vlines(0, 1.1, 0, colors=\"gray\", linestyles=\"--\")\n",
    "plt.xlim(-1.5, batching_sizes[len(batching_sizes)-1]+0.5)\n",
    "plt.ylim(0,1.1)\n",
    "plt.grid(which='major', color='#666666', linestyle='--', alpha = 0.8)\n",
    "plt.grid(which='minor', color='#666666', linestyle='--', alpha = 0.3)\n",
    "plt.minorticks_on()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mailcom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
